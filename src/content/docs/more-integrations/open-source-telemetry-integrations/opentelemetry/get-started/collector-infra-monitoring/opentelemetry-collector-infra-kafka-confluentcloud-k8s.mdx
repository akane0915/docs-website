---
title: Collector for Confluent Cloud and Kafka monitoring
tags:
  - Integrations
  - Open source telemetry integrations
  - OpenTelemetry
  - Kafka
  - Confluent Cloud
  - kubernetes
  - helm
metaDescription: You can collect Kafka metrics from Confluent using the OpenTelemetry Collector on Kubernetes.
freshnessValidatedDate: 2024-05-28
---

You can collect metrics about your Confluent Cloud-managed Kafka deployment with the OpenTelemetry Collector. The collector is a component of OpenTelemetry that collects, processes, and exports telemetry data to New Relic (or any observability backend).

This integration works by running a Prometheus receiver configuration inside the OpenTelemetry collector, which scrapes [Confluent Cloud's metrics API](https://api.telemetry.confluent.cloud/docs/descriptors/datasets/cloud?_ga=2.183807142.1264186867.1705940186-6520871.1686857317&_gl=1*1te8jue*_ga*NjUyMDg3MS4xNjg2ODU3MzE3*_ga_D2D3EGKSGD*MTcwNjAzNzYwOS41Ni4wLjE3MDYwMzc2MDkuNjAuMC4w) and exports that data to New Relic.

## Set up monitoring [#set-up-monitoring]

Complete the steps below to collect Kafka metrics from Confluent and export them to New Relic.

<Steps>
    <Step>
      ### Make sure you're ready to get started [#get-started] 

      Before you start, you need to have the <InlinePopover type="licenseKey" /> for the account you want to report data to. You should also verify that:

      * You have a Kubernetes cluster running
      * (For Helm users) Make sure you have [Helm](https://helm.sh/docs/intro/quickstart/) installed
      * You have a [Confluent Cloud account](https://www.confluent.io/get-started/) with a cluster running
      * You have your [Confluent Cloud API key & secret](https://docs.confluent.io/confluent-cli/current/command-reference/api-key/confluent_api-key_create.html) available

    </Step>
    <Step>
      ### Download or clone the OpenTelemetry tech partner examples repository [#download-repo]

      Download [New Relic's OpenTelemetry tech partnership examples repository](https://github.com/newrelic-experimental/tech-partner-opentelemetry-integrations) as this setup uses its example collector configuration. Once installed, open the [Confluent Cloud example](https://github.com/newrelic-experimental/tech-partner-opentelemetry-integrations/confluent-cloud) directory. For more information, you can check the `README` there as well.

    </Step>
    <Step>
      ### Run the example [#run-the-example]
    
      Complete either the Helm or Kubernetes instructions below, depending on your environment.
      
      #### Helm example [#run-helm-example]

      1. From the `./confluent-cloud/helm` directory, update the variables in the `values.yaml` file. For more information on the individual variables, see the [local variables](#local-variables) table below.

      2. Run the collector by using the following command:

          ```bash
          # Open the confluent cloud helm example directory
          cd ./confluent-cloud/helm

          # Make sure to set environment variables.

          # Install and run helm
          helm install ./helm
          ```


      #### Kubernetes example [#run-kubernetes-example]

      1. From the `./confluent-cloud/k8s` directory, update the variables in the in the `./k8s/deployment.yaml` file. For more information on the individual variables, see the [local variables](#local-variables) table below.

      2. Once the variables are set, run the following command to apply the collector to your Kubernetes cluster.

          ```bash
          # Open the Confluent Cloud k8s example directory
          cd ./confluent-cloud/k8s

          # Make sure to set environment variables

          # Apply the collector to the cluster
          kubectl apply -f ./k8s
          ```


      #### Local variables for Helm and Kubernetes [#local-variables]

        <table>
          <thead>
            <tr>
              <th style={{ width: "275px"}}>
                Variable
              </th>
              <th>
                Description
              </th>
              <th>
                Docs
              </th>
            </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  `NEW_RELIC_API_KEY`
                </td>
                <td>
                  New Relic Ingest API Key
                </td>
                <td>
                  [API Key docs](/docs/apis/intro-apis/new-relic-api-keys/)
                </td>
              </tr>
              <tr>
                <td>
                  `NEW_RELIC_OTLP_ENDPOINT`
                </td>
                <td>
                  Default US New Relic OTLP endpoint is https://otlp.nr-data.net:4318
                </td>
                <td>
                  [OTLP endpoint config docs](/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/best-practices/opentelemetry-otlp)
                </td>
              </tr>
              <tr>
                <td>
                  `CLUSTER_ID`
                </td>
                <td>
                  ID of cluster from Confluent Cloud
                </td>
                <td>
                  [Docs for list cluster ID command](https://docs.confluent.io/confluent-cli/current/command-reference/kafka/cluster/confluent_kafka_cluster_list.html#description)
                </td>
              </tr>
              <tr>
                <td>
                  `CONFLUENT_API_KEY`
                </td>
                <td>
                  Cloud API key
                </td>
                <td>
                  [Cloud API key docs](https://docs.confluent.io/cloud/current/monitoring/metrics-api.html#metrics-quick-start)
                </td>
              </tr>
              <tr>
                <td>
                  `CONFLUENT_API_SECRET`
                </td>
                <td>
                  Cloud API secret
                </td>
                <td>
                  [Cloud API key docs](https://docs.confluent.io/cloud/current/monitoring/metrics-api.html#metrics-quick-start)
                </td>
              </tr>
              <tr>
                <td>
                  `CONNECTOR_ID`
                </td>
                <td>
                  (OPTIONAL) You can monitor your Confluent connectors by specifying the ID here
                </td>
                <td>
                  [Docs for list connector ID command](https://docs.confluent.io/confluent-cli/current/command-reference/connect/cluster/confluent_connect_cluster_list.html)
                </td>
              </tr>
              <tr>
                <td>
                  `SCHEMA_REGISTRY_ID`
                </td>
                <td>
                  (OPTIONAL) You can monitor your Confluent Schema Registry by specifying the ID here
                </td>
                <td>
                  [Docs for list connector ID command](https://docs.confluent.io/confluent-cli/current/command-reference/schema-registry/schema/confluent_schema-registry_schema_list.html)
                </td>
              </tr>
            </tbody>
        </table>

    </Step>
    <Step>

      ### View your data in New Relic [#view-your-data]

      You can view your Confluent Cloud data in a few different ways.

      * Navigate to the [New Relic marketplace](https://one.newrelic.com/marketplace) and search for `Confluent`. You can install the available dashboards right onto your account!
      * Navigate to the metrics explorer and filter for `confluent_kafka`. This data can be added to any custom alert or dashboard.


    </Step>
</Steps>

## Confluent Cloud metrics [#confluent-metrics]

This integration covers all the *Exportable* metrics within the [Confluent Cloud Metrics API](https://api.telemetry.confluent.cloud/docs/descriptors/datasets/cloud). We have a partial list of the *Exportable* metrics below:

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Name
      </th>
      <th>
        Description
      </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        confluent_kafka_server_received_bytes
      </td>
      <td>
        The delta count of bytes of your data received from the network. Each sample is the number of bytes received since the previous data sample. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_sent_bytes
      </td>
      <td>
        The delta count of bytes of your data sent over the network. Each sample is the number of bytes sent since the previous data point. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_received_records
      </td>
      <td>
        The delta count of records received. Each sample is the number of records received since the previous data sample. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_sent_records
      </td>
      <td>
        The delta count of records sent. Each sample is the number of records sent since the previous data point. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_retained_bytes
      </td>
      <td>
        The current count of bytes retained by the cluster. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_active_connection_count
      </td>
      <td>
        The count of active authenticated connections.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_request_count
      </td>
      <td>
        The delta count of requests received over the network. Each sample is the number of requests received since the previous data point. The count sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_partition_count
      </td>
      <td>
        The number of partitions
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_successful_authentication_count
      </td>
      <td>
        The delta count of successful authentications. Each sample is the number of successful authentications since the previous data point. The count sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_consumer_lag_offsets
      </td>
      <td>
        The lag between a group member's committed offset and the partition's high watermark.
      </td>
    </tr>
  </tbody>
</table>


